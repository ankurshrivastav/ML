{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Review Summary Generator\n",
    "\n",
    "# Approach\n",
    "\n",
    "1. Import packages. Remember to import tensorflow keras. Check version of Tf, should be 2.0 or above\n",
    "2. Bring all util functions in the main notebook\n",
    "3. Load 50D glove embedding\n",
    "4. Intialize global variables. These include\n",
    "\n",
    "    - m = No. of training + test records to be loaded (m)\n",
    "    - Tx = Max number of words in a review \n",
    "    - Ty = max number of words in the summary\n",
    "    - vocab_size = number of words in the vocanbulary\n",
    "    \n",
    "5. Implement Sentence_to_indices. Clean-up unwanted chars in the words\n",
    "6. Import Review & Summary data to get X,Y\n",
    "7. Convert Y_Indices to One Hot\n",
    "8. Split X & Y into test & train sets\n",
    "9. Create the Embedding Layer\n",
    "10. Create rest of the Model. Feed the embedding layer output to the rest of the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps to Try\n",
    "\n",
    "1. Try Sparse Cross Entrop Loss - https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other\n",
    "2. See if some words can be stripped off from the vocanbulary (liek special characters). This will reduce vocab_size and thus help y_oh computation\n",
    "3. Add Dropout Regularization to the Model\n",
    "4. Increase Tx - that is allow more words in the review\n",
    "5. Go through the one step attention model video. Tweak n_a & n_s parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib\n",
    "import re\n",
    "import sklearn.model_selection \n",
    "\n",
    "\n",
    "#importlib.reload(nmt_utils)\n",
    "#importlib.reload(emo_utils)\n",
    "\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding, RepeatVector, Concatenate, Dot, Bidirectional\n",
    "#from tensorflow.keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.preprocessing import *\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "#from emo_utils import *\n",
    "#from nmt_utils import *\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Functions\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "# Read data file to get training & test data\n",
    "def read_review_data(filename,m):\n",
    "    review = []\n",
    "    summary = []\n",
    "    \n",
    "    ctr = 0\n",
    "\n",
    "    review_summary_data = []\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "         \n",
    "\n",
    "        for row in csvReader:\n",
    "            #review.append(row[1])\n",
    "            #summary.append(row[0])\n",
    "            #row = [d.replace('\"', '') for d in data]\n",
    "            \n",
    "            review_summary_data.append((row[1], row[0])) \n",
    "            #print(\"Review is \", row[1])\n",
    "            #print(\"Summary is \", row[0])\n",
    "            \n",
    "            ctr = ctr + 1\n",
    "            if ctr<m:\n",
    "                if ctr%(m*0.1)==0:\n",
    "                    print(\"Number of reviews loaded \", ctr)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    #X = np.asarray(review)\n",
    "    #Y = np.asarray(summary)     \n",
    "\n",
    "    #return X, Y\n",
    "    \n",
    "    \n",
    "    return np.array(review_summary_data)\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    #make lower to standardize\n",
    "    string = str(string)\n",
    "    string = string.lower()\n",
    "    \n",
    "    \n",
    "    string = string.replace('\\'','')\n",
    "    string = string.replace('\"','')\n",
    "    #print(string)\n",
    "    \n",
    "    \n",
    "        \n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    #print(\"String length is  \", len(string))\n",
    "    \n",
    "    rep=[]  \n",
    "    string = string.split()\n",
    "    #print(\"number of words in string are \", len(string))\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        #print(\"String length after adjustment is  \", len(string))\n",
    "        \n",
    "    \n",
    "    #rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    #rep = list(map(lambda x: vocab.get(x), string))\n",
    "    for word_index in range (0,len(string)):\n",
    "        idx = vocab.get(string[word_index])\n",
    "        if idx:\n",
    "            rep.append(idx)\n",
    "    \n",
    "    #print(\"length of rep indices list is \", len(rep))\n",
    "    #print(\"Inside string_to_int, rep is \", rep)\n",
    "    \n",
    "    # Pad the remaining places in the sentence with <pad>\n",
    "    if len(rep) < length:\n",
    "        rep += [vocab['<pad>']] * (length - (len(rep)))\n",
    "        #print(\"length of rep indices list after padding with <pad> is \", len(rep))  \n",
    "    \n",
    "          \n",
    "    #print (rep)\n",
    "    return rep\n",
    "\n",
    "def preprocess_data(dataset, word_to_index, Tx, Ty,m):\n",
    "    \n",
    "    X1, Y1 = zip(*dataset)\n",
    "    #print(\"X1 inside preprocess data \", X1)\n",
    "    \n",
    "    #X = str(X).split()\n",
    "    #Y = str(Y).split()\n",
    "    \n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    X,Y= np.empty((m,Tx)),np.empty((m,Ty))     \n",
    "    \n",
    "    #X = [string_to_int(a, Tx, word_to_index) for a in X1]\n",
    "    #Y = [string_to_int(t, Ty, word_to_index) for t in Y1]\n",
    "    \n",
    "    ctr = 0\n",
    "    for sentence in X1:\n",
    "        indices_X = np.array(string_to_int(sentence, Tx, word_to_index))\n",
    "        # debug\n",
    "        #if indices_X.shape[0]!=Tx:\n",
    "            #print(\"indices_X shape !=50 for sentence \", sentence)\n",
    "            #print(\"indices_X shape !=50 at position \", ctr)\n",
    "            #print(\"indices_X shape !=50 , shape is  \", indices_X.shape[0])\n",
    "        \n",
    "        X[ctr] = indices_X\n",
    "        ctr = ctr + 1\n",
    "        \n",
    "        \n",
    "    ctr = 0\n",
    "    for sentence in Y1:\n",
    "        indices_Y = string_to_int(sentence, Ty, word_to_index) \n",
    "        Y[ctr] = indices_Y\n",
    "        ctr = ctr + 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    #Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    #Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    #return X, np.array(Y)\n",
    "    \n",
    "    #print(\"X shape from preprocess _data is \", (np.array(X)).shape)\n",
    "    #print(\"Y shape from preprocess _data is \", (np.array(Y)).shape)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def convert_indices_to_words(idx_array,dictionary):\n",
    "    output = []\n",
    "    #print(\"idx_array \", idx_array)\n",
    "    for word_idx in idx_array:\n",
    "        word = dictionary.get(int(word_idx))\n",
    "        #print(word_idx)\n",
    "        if word!=None:\n",
    "            output.append(word)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Step # 3\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../glove.6B/glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "\n",
    "m = 20 # Remember that the 1 record (header record) from the raw data file will be popped. So set m to be 1 more that the train + test records required\n",
    "Tx = 40 # Maximum of 40 words in a review\n",
    "Ty = 5 # Maximum 5 words in the summary of the review\n",
    "\n",
    "# Add <pad> and '<unk> tokens to all the lists\n",
    "index_to_word[0]=\"<pad>\" # End of sequence word\n",
    "word_to_index[\"<pad>\"]=0\n",
    "word_to_vec_map[\"<pad>\"] = np.zeros((1,50))\n",
    "\n",
    "# In the original glove.6B file, index = 4 had \"!!!!\". Since its extremely rare & ununsed, replacing this index position with \"<unk>\"\n",
    "\n",
    "index_to_word[4]=\"<unk>\" # End of sequence word\n",
    "word_to_index[\"<unk>\"]=4\n",
    "word_to_vec_map[\"<unk>\"] = np.zeros((1,50))\n",
    "\n",
    "vocab_size = len(word_to_index)+1 #Since <pad> has been added (nothing existed at index = 0) and \"unk\" has been swapped\n",
    "\n",
    "#print(word_to_index[\"None\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dset)  3\n",
      "X_Indices.shape  (20, 40)\n",
      "Y_Indices.shape  (20, 5)\n",
      "dataset[0]  This is the third review\n",
      "dataset[1]  Awesome product\n",
      "\n",
      "Source after preprocessing (indices): [358160. 192973. 357266. 358029. 307253.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.]\n",
      "Target after preprocessing (indices): [ 64354. 292984.      0.      0.      0.]\n",
      "\n",
      "sentence x is  ['this', 'is', 'the', 'third', 'review', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "sentence y is  ['awesome', 'product', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Unit testing review & summary pre-processing \n",
    "\n",
    "dset = read_review_data('file_for_testing_pre_processing.csv',3)\n",
    "\n",
    "print(\"len(dset) \", len(dset))\n",
    "\n",
    "\n",
    "\n",
    "#dataset= [[X_data,Y_data]]\n",
    "#for i in range(1,m):\n",
    "    #dataset.append((X_data[i], Y_data[i]))\n",
    "\n",
    "X_Indices, Y_Indices = preprocess_data(dset, word_to_index, Tx, Ty,m)\n",
    "\n",
    "print(\"X_Indices.shape \", X_Indices.shape)\n",
    "print(\"Y_Indices.shape \", Y_Indices.shape)\n",
    "\n",
    "#print(\"X_Indices[0,0:10]  \", X_Indices[0][1])\n",
    "\n",
    "# print & check the source & target conversion to indices\n",
    "index = 2\n",
    "\n",
    "print(\"dataset[0] \", dset[index][0])\n",
    "print(\"dataset[1] \", dset[index][1])\n",
    "#print(\"Source date:\", dataset[index][0])\n",
    "#print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X_Indices[index])\n",
    "print(\"Target after preprocessing (indices):\", Y_Indices[index])\n",
    "print()\n",
    "\n",
    "print(\"sentence x is \", convert_indices_to_words(X_Indices[index],index_to_word))\n",
    "print(\"sentence y is \", convert_indices_to_words(Y_Indices[index],index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews loaded  2\n",
      "Number of reviews loaded  4\n",
      "Number of reviews loaded  6\n",
      "Number of reviews loaded  8\n",
      "Number of reviews loaded  10\n",
      "Number of reviews loaded  12\n",
      "Number of reviews loaded  14\n",
      "Number of reviews loaded  16\n",
      "Number of reviews loaded  18\n",
      "len(dataset)  20\n",
      "X_Indices.shape  (20, 40)\n",
      "Y_Indices.shape  (20, 5)\n",
      "dataset[0]  Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You\n",
      "dataset[1]  GREAT SWEET CANDY!\n",
      "\n",
      "Source after preprocessing (indices): [368318. 344429. 254258.  98992. 145839.  90792. 229835. 188481. 216635.\n",
      " 280746.  88126. 392261. 314370.  90755. 188692. 269953. 268046. 357266.\n",
      " 269091. 107031. 148889. 188481. 357266. 372191. 342019. 264937.  43010.\n",
      " 346136. 268046. 357266. 177567. 106170. 357266. 106170. 383514. 139954.\n",
      " 188481.  13194.  60665. 394521.]\n",
      "Target after preprocessing (indices): [166369. 349415.  90792.      0.      0.]\n",
      "\n",
      "sentence x is  ['twizzlers', 'strawberry', 'my', 'childhood', 'favorite', 'candy', 'made', 'in', 'lancaster', 'pennsylvania', 'by', 'y', 's', 'candies', 'inc', 'one', 'of', 'the', 'oldest', 'confectionery', 'firms', 'in', 'the', 'united', 'states', 'now', 'a', 'subsidiary', 'of', 'the', 'hershey', 'company', 'the', 'company', 'was', 'established', 'in', '1845', 'as', 'young']\n",
      "sentence y is  ['great', 'sweet', 'candy', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Load test & training data\n",
    "\n",
    "dataset = []\n",
    "#X_data, Y_data = read_review_data('product_reviews_modified.csv',m)\n",
    "\n",
    "dataset = read_review_data('product_reviews_modified.csv',m)\n",
    "# Remove the header rows\n",
    "#X_data.pop(0)\n",
    "#Y_data.pop(0)\n",
    "\n",
    "#print(\"Number of Reviews & Summaries Loaded = \", len(X_train), len(Y_train))\n",
    "\n",
    "#print(X_data[1], Y_data[1])\n",
    "#print(\"X_data shape is \", X_data.shape)\n",
    "\n",
    "print(\"len(dataset) \", len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "#dataset= [[X_data,Y_data]]\n",
    "#for i in range(1,m):\n",
    "    #dataset.append((X_data[i], Y_data[i]))\n",
    "\n",
    "X_Indices, Y_Indices = preprocess_data(dataset, word_to_index, Tx, Ty,m)\n",
    "\n",
    "print(\"X_Indices.shape \", X_Indices.shape)\n",
    "print(\"Y_Indices.shape \", Y_Indices.shape)\n",
    "\n",
    "#print(\"X_Indices[0,0:10]  \", X_Indices[0][1])\n",
    "\n",
    "# print & check the source & target conversion to indices\n",
    "index = m-1\n",
    "\n",
    "print(\"dataset[0] \", dataset[index][0])\n",
    "print(\"dataset[1] \", dataset[index][1])\n",
    "#print(\"Source date:\", dataset[index][0])\n",
    "#print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X_Indices[index])\n",
    "print(\"Target after preprocessing (indices):\", Y_Indices[index])\n",
    "print()\n",
    "\n",
    "print(\"sentence x is \", convert_indices_to_words(X_Indices[index],index_to_word))\n",
    "print(\"sentence y is \", convert_indices_to_words(Y_Indices[index],index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[279682])\n",
    "print(word_to_index['were'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of labels, if required\n",
    "\n",
    "#start1 = time.process_time()\n",
    "#https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "#Y_oh = tf.one_hot(Y_train_indices,vocab_size,on_value=None,off_value=None,axis=-1,dtype=tf.int8,name='One Hot')\n",
    "#print(\"Y_oh.shape \", Y_oh.shape)\n",
    "#print(\"Time taken in sec \", time.process_time() - start1) # Should take 3.5 seconds for 500000 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape  (20, 40)\n",
      "Y.shape  (20, 5)\n",
      "Training data shape  (14, 40) (14, 5)\n",
      "Test data shape  (6, 40) (6, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split data into test & train\n",
    "\n",
    "train_size = 0.7\n",
    "train_records = round(train_size*m)\n",
    "X = np.array(X_Indices)\n",
    "Y = np.array(Y_Indices) #Y_oh\n",
    "\n",
    "print(\"X.shape \", X.shape)\n",
    "print(\"Y.shape \", Y.shape)\n",
    "\n",
    "X_train_indices, X_test_indices = X[0:train_records,:],X[train_records:,:]\n",
    "y_train_oh, y_test_oh = Y[0:train_records,:],Y[train_records:,:]\n",
    "\n",
    "print(\"Training data shape \", X_train_indices.shape,y_train_oh.shape )\n",
    "print(\"Test data shape \", X_test_indices.shape,y_test_oh.shape )\n",
    "\n",
    "#y_train_oh, y_test_oh = train_test_split(X_train_indices, Y_oh, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "# Check the function. Expected output should be **weights[0][1][3] =**\t-0.3403\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects required for the One step attention part of the model\n",
    "\n",
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(Ty, activation = \"tanh\")# Was Dense(10, activation = \"tanh\") in the week 3 course. In that course Ty=10, so this is updated to Ty\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    #print(s_prev.shape[1])\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters required for main model\n",
    "\n",
    "n_a = 32 # Was 32 in the week 3 course\n",
    "n_s = 64 # Was 64 in the week 3 course\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(vocab_size, activation=softmax)\n",
    "#output_layer = Dense(Ty, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(input_shape,word_to_vec_map, word_to_index,Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    \n",
    "    \n",
    "    # X is commented out because the input to the model has to be sentence_indices\n",
    "    #X = Input(shape=(Tx, human_vocab_size))\n",
    "    \n",
    "    \n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    y0 = Input(shape=(n_s,), name='y0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    yt = y0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create the Main Model\n",
    "\n",
    "    # This should be the initial part of the model that starts with the embedding layer\n",
    "\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    #input_shape = (maxlen,) (in emojify example) which is further = Tx (Maximum words in the input sequence)\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    \n",
    "    ##### The following lines of code have to be merged into one line. ###########\n",
    "    #Embeddings have to be the input to the Bidirectional LSTM\n",
    "    \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    # The resulting output of the Bidirectional LSTM should be 'a'\n",
    "    #X = LSTM(128, return_sequences=True, return_state = False)(embeddings)\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    #a = Bidirectional(LSTM(n_a, return_sequences=True),input_shape=X.shape)(X)\n",
    "    \n",
    "    \n",
    "    # Merged layer, from Emojiy & Machine Tranlation (Week 3 Excercise)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### End of Merge. \n",
    "    \n",
    "    out=[]\n",
    "    \n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s,_,c = post_activation_LSTM_cell(context, initial_state = [s,c])\n",
    "        #s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs=[sentence_indices ,s0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# Input value X should be of shape (m,Tx,). In Emojofy, sentences_to_indices returns arrays. \n",
    "# In the implementation above, the return type has been changed to a list. If there is an error, try converting the list to array (m,Tx,)\n",
    "print(Tx)\n",
    "model = model((Tx,), word_to_vec_map, word_to_index,Tx, Ty, n_a, n_s, vocab_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 50)       20000150    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 64)       21248       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 40, 64)       0           s0[0][0]                         \n",
      "                                                                 unified_lstm_1[5][0]             \n",
      "                                                                 unified_lstm_1[6][0]             \n",
      "                                                                 unified_lstm_1[7][0]             \n",
      "                                                                 unified_lstm_1[8][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 40, 5)        645         concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40, 1)        6           dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 40, 1)        0           dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM)    [(None, 64), (None,  33024       dot[5][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 unified_lstm_1[5][0]             \n",
      "                                                                 unified_lstm_1[5][2]             \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 unified_lstm_1[6][0]             \n",
      "                                                                 unified_lstm_1[6][2]             \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 unified_lstm_1[7][0]             \n",
      "                                                                 unified_lstm_1[7][2]             \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 unified_lstm_1[8][0]             \n",
      "                                                                 unified_lstm_1[8][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400003)       26000195    unified_lstm_1[5][0]             \n",
      "                                                                 unified_lstm_1[6][0]             \n",
      "                                                                 unified_lstm_1[7][0]             \n",
      "                                                                 unified_lstm_1[8][0]             \n",
      "                                                                 unified_lstm_1[9][0]             \n",
      "==================================================================================================\n",
      "Total params: 46,055,268\n",
      "Trainable params: 26,055,118\n",
      "Non-trainable params: 20,000,150\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.01,  beta_1=0.9,beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 50)\n",
      "y_train_oh.shape  (14000, 5)\n",
      "Epoch 1/5\n",
      "14000/14000 [==============================] - 272s 19ms/sample - loss: 29.3542 - dense_2_loss: 3.9351 - dense_2_accuracy: 0.0299 - dense_2_accuracy_1: 0.1111 - dense_2_accuracy_2: 0.3277 - dense_2_accuracy_3: 0.5064 - dense_2_accuracy_4: 0.6695\n",
      "Epoch 2/5\n",
      "14000/14000 [==============================] - 267s 19ms/sample - loss: 24.4409 - dense_2_loss: 2.9754 - dense_2_accuracy: 0.0984 - dense_2_accuracy_1: 0.1116 - dense_2_accuracy_2: 0.3303 - dense_2_accuracy_3: 0.5103 - dense_2_accuracy_4: 0.6745\n",
      "Epoch 3/5\n",
      "14000/14000 [==============================] - 269s 19ms/sample - loss: 23.9354 - dense_2_loss: 2.8901 - dense_2_accuracy: 0.0984 - dense_2_accuracy_1: 0.1116 - dense_2_accuracy_2: 0.3303 - dense_2_accuracy_3: 0.5103 - dense_2_accuracy_4: 0.6745\n",
      "Epoch 4/5\n",
      "14000/14000 [==============================] - 271s 19ms/sample - loss: 23.7220 - dense_2_loss: 2.8508 - dense_2_accuracy: 0.0984 - dense_2_accuracy_1: 0.1116 - dense_2_accuracy_2: 0.3303 - dense_2_accuracy_3: 0.5103 - dense_2_accuracy_4: 0.6745\n",
      "Epoch 5/5\n",
      "14000/14000 [==============================] - 272s 19ms/sample - loss: 23.6084 - dense_2_loss: 2.8315 - dense_2_accuracy: 0.0984 - dense_2_accuracy_1: 0.1116 - dense_2_accuracy_2: 0.3303 - dense_2_accuracy_3: 0.5103 - dense_2_accuracy_4: 0.6745\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(X_train_indices.shape)\n",
    "print(\"y_train_oh.shape \", y_train_oh.shape)\n",
    "#print(\"tf.transpose(y_train_oh,perm=[1,0,2] \", tf.transpose(y_train_oh,perm=[1,0,2])\n",
    "s0 = np.zeros((train_records, n_s))\n",
    "c0 = np.zeros((train_records, n_s))\n",
    "\n",
    "#https://www.tensorflow.org/api_docs/python/tf/transpose. #See the description in the week 3 course\n",
    "#outputs = list(tf.transpose(y_train_oh,perm=[1,0,2]))\n",
    "outputs = list(tf.transpose(y_train_oh,perm=[1,0]))\n",
    "#outputs = y_train_oh\n",
    "\n",
    "# Recreate the exact same model purely from the file\n",
    "#model = keras.models.load_model('model_name.h5')\n",
    "\n",
    "\n",
    "history = model.fit([X_train_indices, s0, c0], outputs, epochs=5, batch_size=100)\n",
    "\n",
    "model_name = \"model\" + datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")+\".h5\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 81s 13ms/sample - loss: 25.7968 - dense_2_loss: 3.1464 - dense_2_accuracy: 0.0978 - dense_2_accuracy_1: 0.1070 - dense_2_accuracy_2: 0.3073 - dense_2_accuracy_3: 0.4920 - dense_2_accuracy_4: 0.6630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.796771144866945,\n",
       " 3.1464252,\n",
       " 3.1464252,\n",
       " 3.1464252,\n",
       " 3.1464252,\n",
       " 3.1464252,\n",
       " 0.097833335,\n",
       " 0.107,\n",
       " 0.30733332,\n",
       " 0.492,\n",
       " 0.663]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the accuracy against the test set\n",
    "\n",
    "\n",
    "#outputs_test = outputs_test[0:m-train_records,:,:]\n",
    "#outputs_test = list(tf.transpose(y_test_oh,perm=[1,0,2]))\n",
    "outputs_test = list(tf.transpose(y_test_oh,perm=[1,0]))\n",
    "\n",
    "s0_test = np.zeros((X_test_indices.shape[0], n_s))\n",
    "c0_test = np.zeros((X_test_indices.shape[0], n_s))\n",
    "#s0_test = np.zeros((m-train_records+1, n_s))\n",
    "#c0_test = np.zeros((m-train_records+1, n_s))\n",
    "\n",
    "model.evaluate([X_test_indices, s0_test,c0_test], outputs_test, batch_size=100, verbose=1, sample_weight=None, steps=None, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(index_to_word[166369])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_test_idx.shape after tf.permute[1,0] (10, 5)\n",
      "prediction idx for the record prediction_test_idx[0:1,:] tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "['what', 'i', 'like', 'about', 'these', 'first', 'and', 'foremost', 'is', 'that', 'they', 'are', 'healthy', 'and', 'organic', 'and', 'not', 'stuffed', 'full', 'of', 'preservatives', 'i', 'also', 'trust', 'newman', 'products', 'and', 'have', 'always', 'loved', 'the', 'salad', 'dressings', 'and', 'pasta', 'sauces', 'they', 'make', 'for', 'me', 'not', 'my', 'dog', 'top', 'quality', 'br', 'at', 'first', 'my', 'dog']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  0\n",
      "['my', 'malamutes', 'arent', 'too', 'picky', 'except', 'one', 'who', 'hates', 'peanut', 'butter', 'so', 'i', 'decided', 'to', 'try', 'these', 'treats', 'very', 'hard', 'and', 'crunchy', 'biscuits', 'of', 'a', 'decent', 'size', 'if', 'i', 'have', 'one', 'complaint', 'theyre', 'so', 'hard', 'i', 'cant', 'break', 'them', 'in', 'two', 'and', 'have', 'to', 'give', 'the', 'whole', 'treat', 'to', 'my']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  1\n",
      "['completely', 'non', 'gmo', 'this', 'is', 'a', 'usda', 'organic', 'treat', 'from', 'new', 'zealand', 'organics', 'ingredients', 'in', 'order', 'on', 'the', 'package', 'are', 'barley', 'lamb', 'carrots', 'apples', 'palm', 'oil', 'mixed', 'tocopherols', 'vitamin', 'e', 'and', 'molasses', 'its', 'certified', 'by', 'oregon', 'tilth', 'not', 'one', 'of', 'the', 'factory', 'certification', 'corporations', 'like', 'qai', 'br', 'br', 'my', '<pad>']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  2\n",
      "['ok', 'so', 'i', 'may', 'not', 'know', 'how', 'exactly', 'my', 'dogs', 'feel', 'about', 'the', 'taste', 'of', 'these', 'treats', 'i', 'do', 'know', 'that', 'my', 'dogs', 'snapped', 'these', 'up', 'and', 'went', 'to', 'their', 'perspective', 'corners', 'to', 'enjoy', 'these', 'treats', 'going', 'to', 'your', 'special', 'place', 'to', 'enjoy', 'a', 'treat', 'at', 'my', 'house', 'only', 'happens']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  3\n",
      "['i', 'gave', 'these', 'product', '4', 'stars', 'because', 'our', '2', 'schnauzers', 'med', 'small', 'loved', 'it', 'however', 'the', 'snacks', 'are', 'extremely', 'hard', 'our', 'med', 'size', 'schnauzer', 'was', 'able', 'to', 'finally', 'break', 'one', 'up', 'and', 'he', 'enjoyed', 'it', 'thoroughly', 'but', 'our', 'smaller', 'schnauzer', 'couldnt', 'even', 'think', 'about', 'trying', 'to', 'eat', 'it', 'my', 'husband']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  4\n",
      "['pit', 'terrier', 'and', 'loved', 'these', 'a', 'bit', 'more', 'than', 'the', 'other', 'treats', 'we', 'had', 'was', 'it', 'the', 'ranch', 'flavor', 'was', 'it', 'the', 'texture', 'we', 'can', 'only', 'speculate', 'but', 'they', 'know', 'for', 'sure', 'oooh', 'those', 'treats', 'they', 'love', 'to', 'munch', 'grama', 'even', 'ate', 'one', 'by', 'mistake', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  5\n",
      "['my', 'enjoys', 'these', 'dog', 'treats', 'after', 'he', 'sits', 'to', 'get', 'one', 'he', 'carries', 'it', 'off', 'to', 'another', 'room', 'to', 'eat', 'it', 'this', 'is', 'because', 'the', 'cookie', 'is', 'a', 'bit', 'large', 'and', 'he', 'wants', 'to', 'enjoy', 'it', 'in', 'private', 'that', 'as', 'far', 'as', 'i', 'can', 'see', 'is', 'the', 'only', 'downside', '<pad>']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  6\n",
      "['i', 'own', 'a', '7', 'year', 'old', 'miniature', 'australian', 'sheppard', 'hes', 'about', '22', 'lbs', 'he', 'seems', 'to', 'really', 'like', 'the', 'taste', 'of', 'these', 'biscuits', 'he', 'does', 'have', 'a', 'bit', 'of', 'difficulty', 'eating', 'them', 'though', 'they', 'are', 'quite', 'thick', 'in', 'size', 'most', 'dog', 'biscuits', 'i', 'give', 'him', 'are', 'less', 'than', '1', '2']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  7\n",
      "['these', 'treats', 'are', 'great', 'for', 'those', 'who', 'are', 'particular', 'about', 'what', 'they', 'feed', 'their', 'dog', 'we', 'do', 'a', 'wheat', 'corn', 'free', 'diet', 'and', 'have', 'tried', 'other', 'treats', 'from', 'the', 'newmans', 'own', 'line', 'our', 'dog', 'a', 'lab', 'shepherd', 'rescue', 'has', 'loved', 'them', 'all', 'but', 'most', 'of', 'the', 'others', 'are', 'a', '<pad>']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  8\n",
      "['i', 'have', 'two', 'dogs', 'a', '5', 'year', 'old', 'weimaraner', 'who', 'eats', 'everything', 'except', 'parsley', 'and', 'the', 'colored', 'parts', 'of', 'citrus', 'rinds', 'even', 'rocks', 'and', 'especially', 'sticks', 'and', 'a', '12', 'year', 'old', 'german', 'shepherd', 'dog', 'who', 'eats', 'as', 'little', 'as', 'i', 'let', 'her', 'get', 'away', 'with', 'she', 'doesnt', 'even', 'like', 'fish']\n",
      "[None, None, None, None, None]\n",
      "tf.Tensor([166369      0      0      0      0], shape=(5,), dtype=int64)\n",
      "Finished test output number  9\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions against the test set and see how they match up with the true labels of the same test set\n",
    "word = np.empty((50,5),dtype=np.object)\n",
    "\n",
    "prediction_test = model.predict([X_test_indices[0:10,:], s0, c0])\n",
    "prediction_test_idx = np.argmax(prediction_test, axis=-1)\n",
    "\n",
    "prediction_test_idx= tf.transpose(prediction_test_idx,perm=[1,0])\n",
    "print(\"prediction_test_idx.shape after tf.permute[1,0]\", prediction_test_idx.shape) \n",
    "print(\"prediction idx for the record prediction_test_idx[0:1,:]\", prediction_test_idx[3,:])\n",
    "\n",
    "pred_list = list(prediction_test_idx)\n",
    "#print(pred_list)\n",
    "\n",
    "#for record in range(1:50):\n",
    "for i in range(0,10):\n",
    "    X = X_test_indices[i,:]\n",
    "    x_wrd_keys = [index_to_word.get(a) for a in X]\n",
    "    print (x_wrd_keys)\n",
    "    \n",
    "    y_pred_keys = [index_to_word.get(b) for b in pred_list[i]]\n",
    "    print (y_pred_keys)\n",
    "    print (prediction_test_idx[i,:])\n",
    "    \n",
    "    print(\"Finished test output number \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EXAMPLES = [\"I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most\"]\n",
    "EXAMPLES = ['Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\"','This is the second review']\n",
    "#print(X_train[1])\n",
    "\n",
    "#for example in EXAMPLES:\n",
    "m_predict = len(EXAMPLES)\n",
    "print(m_predict)\n",
    "\n",
    "\n",
    "\n",
    "dataset_predict = []\n",
    "for i in range(0,m_predict):\n",
    "    dataset_predict.append((EXAMPLES[i], \"y\"))\n",
    "\n",
    "X_predict_Indices, _ = preprocess_data(dataset_predict, word_to_index, Tx, Ty,m_predict)\n",
    "\n",
    "    \n",
    "#source = string_to_int(example, Tx, vocab_size)\n",
    "#source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "prediction = model.predict([X_predict_Indices, s0, c0])\n",
    "\n",
    "prediction_idx = np.argmax(prediction, axis=-1)\n",
    "#prediction_idx = model.predict([x_predict_input, s0, c0])\n",
    "print(prediction[0])\n",
    "\n",
    "print(prediction_idx)\n",
    "\n",
    "\n",
    "    \n",
    "#output = [index_to_word[int(i)] for i in prediction_idx]\n",
    "\n",
    "print(\"Review is:\", EXAMPLES[0])\n",
    "#print(\"output:\", output)\n",
    "\n",
    "print(\"Review  word indices are:\", X_predict_Indices[0])\n",
    "#print(\"output:\", output)\n",
    "\n",
    "\n",
    "#output1 = [index_to_word[int(i)] for i in prediction_idx]\n",
    "#print(\"output1 \", output1)\n",
    "\n",
    "print(\" Review Summary is \")\n",
    "output = []\n",
    "for wrd_idx in prediction_idx:\n",
    "    \n",
    "    print(wrd_idx, end =\" \")\n",
    "    wrd = index_to_word[wrd_idx]\n",
    "    print(wrd, end =\" \")\n",
    "    output.append(wrd)\n",
    "    output.append(\" \")\n",
    "\n",
    "print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[132033])\n",
    "print(index_to_word[350784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(5, size=(2,3))\n",
    "#print(z)\n",
    "print(z.shape)\n",
    "\n",
    "z_ravel = z.ravel()\n",
    "print(\"z_ravel = \", z_ravel)\n",
    "print(\"z = \", z)\n",
    "\n",
    "\n",
    "z_ravel = tf.one_hot(z_ravel,5,on_value=None,off_value=None,axis=-1,dtype=tf.int8,name='One Hot')\n",
    "print(\"z_oh \", z_oh)\n",
    "\n",
    "print(\"z_ravel = \", z_ravel)\n",
    "print(\"z = \", z)\n",
    "#v = np.argmax(z,axis=-1)\n",
    "#print(v.shape)\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_back = np.reshape(z_ravel, (2,3,5))\n",
    "print(\"z_back \", z_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance on tf.one_hot on n dim array vs one dime vector\n",
    "import time\n",
    "\n",
    "z = np.random.randint(5, size=(1000,50))\n",
    "\n",
    "start1 = time.process_time()\n",
    "nd = z\n",
    "print(\"nd shape =\", nd.shape)\n",
    "nd_oh = tf.one_hot(nd,400000,on_value=None,off_value=None,axis=-1,dtype=tf.int8,name='One Hot')\n",
    "print(\"nd_oh shape \", nd_oh.shape)\n",
    "print(\"Time taken for nd_oh \", time.process_time() - start1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = time.process_time()\n",
    "od = z.ravel()\n",
    "print(\"od shape =\", od.shape)\n",
    "od_oh = tf.one_hot(od,400000,on_value=None,off_value=None,axis=-1,dtype=tf.int8,name='One Hot')\n",
    "print(\"od_oh shape \", od_oh.shape)\n",
    "print(\"Time taken for nd_oh \", time.process_time() - start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
