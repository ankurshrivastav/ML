{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "2.2.4-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Base Code From: https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib\n",
    "import re\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "import sklearn.model_selection \n",
    "\n",
    "\n",
    "#importlib.reload(nmt_utils)\n",
    "#importlib.reload(emo_utils)\n",
    "\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding, RepeatVector, Concatenate, Dot, Bidirectional\n",
    "#from tensorflow.keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.preprocessing import *\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "#from emo_utils import *\n",
    "#from nmt_utils import *\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Functions\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "# Read data file to get training & test data\n",
    "def read_review_data(filename,m):\n",
    "    review = []\n",
    "    summary = []\n",
    "    \n",
    "    ctr = 0\n",
    "\n",
    "    review_summary_data = []\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "         \n",
    "\n",
    "        for row in csvReader:\n",
    "            #review.append(row[1])\n",
    "            #summary.append(row[0])\n",
    "            #row = [d.replace('\"', '') for d in data]\n",
    "            \n",
    "            review_summary_data.append((row[1], row[0])) \n",
    "            #print(\"Review is \", row[1])\n",
    "            #print(\"Summary is \", row[0])\n",
    "            \n",
    "            ctr = ctr + 1\n",
    "            if ctr<m:\n",
    "                if ctr%(m*0.1)==0:\n",
    "                    print(\"Number of reviews loaded \", ctr)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    #X = np.asarray(review)\n",
    "    #Y = np.asarray(summary)     \n",
    "\n",
    "    #return X, Y\n",
    "    \n",
    "    \n",
    "    return np.array(review_summary_data)\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    #make lower to standardize\n",
    "    string = str(string)\n",
    "    string = string.lower()\n",
    "    \n",
    "    \n",
    "    string = string.replace('\\'','')\n",
    "    string = string.replace('\"','')\n",
    "    #print(string)\n",
    "    \n",
    "    \n",
    "        \n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    #print(\"String length is  \", len(string))\n",
    "    \n",
    "    rep=[]  \n",
    "    string = string.split()\n",
    "    #print(\"number of words in string are \", len(string))\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        #print(\"String length after adjustment is  \", len(string))\n",
    "        \n",
    "    \n",
    "    #rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    #rep = list(map(lambda x: vocab.get(x), string))\n",
    "    for word_index in range (0,len(string)):\n",
    "        idx = vocab.get(string[word_index])\n",
    "        if idx:\n",
    "            rep.append(idx)\n",
    "    \n",
    "    #print(\"length of rep indices list is \", len(rep))\n",
    "    #print(\"Inside string_to_int, rep is \", rep)\n",
    "    \n",
    "    # Pad the remaining places in the sentence with <pad>\n",
    "    if len(rep) < length:\n",
    "        rep += [vocab['<pad>']] * (length - (len(rep)))\n",
    "        #print(\"length of rep indices list after padding with <pad> is \", len(rep))  \n",
    "    \n",
    "          \n",
    "    #print (rep)\n",
    "    return rep\n",
    "\n",
    "def preprocess_data(dataset, word_to_index, Tx, Ty,m):\n",
    "    \n",
    "    X1, Y1 = zip(*dataset)\n",
    "    #print(\"X1 inside preprocess data \", X1)\n",
    "    \n",
    "    #X = str(X).split()\n",
    "    #Y = str(Y).split()\n",
    "    \n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    X,Y= np.empty((m,Tx)),np.empty((m,Ty))     \n",
    "    \n",
    "    #X = [string_to_int(a, Tx, word_to_index) for a in X1]\n",
    "    #Y = [string_to_int(t, Ty, word_to_index) for t in Y1]\n",
    "    \n",
    "    ctr = 0\n",
    "    for sentence in X1:\n",
    "        indices_X = np.array(string_to_int(sentence, Tx, word_to_index))\n",
    "        # debug\n",
    "        #if indices_X.shape[0]!=Tx:\n",
    "            #print(\"indices_X shape !=50 for sentence \", sentence)\n",
    "            #print(\"indices_X shape !=50 at position \", ctr)\n",
    "            #print(\"indices_X shape !=50 , shape is  \", indices_X.shape[0])\n",
    "        \n",
    "        X[ctr] = indices_X\n",
    "        ctr = ctr + 1\n",
    "        \n",
    "        \n",
    "    ctr = 0\n",
    "    for sentence in Y1:\n",
    "        indices_Y = string_to_int(sentence, Ty, word_to_index) \n",
    "        Y[ctr] = indices_Y\n",
    "        ctr = ctr + 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    #Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    #Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    #return X, np.array(Y)\n",
    "    \n",
    "    #print(\"X shape from preprocess _data is \", (np.array(X)).shape)\n",
    "    #print(\"Y shape from preprocess _data is \", (np.array(Y)).shape)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def convert_indices_to_words(idx_array,dictionary):\n",
    "    output = []\n",
    "    #print(\"idx_array \", idx_array)\n",
    "    for word_idx in idx_array:\n",
    "        word = dictionary.get(int(word_idx))\n",
    "        #print(word_idx)\n",
    "        if word!=None:\n",
    "            output.append(word)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Step # 3\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../glove.6B/glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_vec_map[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "\n",
    "m = 20 # Remember that the 1 record (header record) from the raw data file will be popped. So set m to be 1 more that the train + test records required\n",
    "Tx = 40 # Maximum of 40 words in a review\n",
    "Ty = 5 # Maximum 5 words in the summary of the review\n",
    "\n",
    "# Add <pad> and '<unk> tokens to all the lists\n",
    "index_to_word[0]=\"<pad>\" # End of sequence word\n",
    "word_to_index[\"<pad>\"]=0\n",
    "word_to_vec_map[\"<pad>\"] = np.zeros((1,50))\n",
    "\n",
    "# In the original glove.6B file, index = 4 had \"!!!!\". Since its extremely rare & ununsed, replacing this index position with \"<unk>\"\n",
    "\n",
    "index_to_word[4]=\"<unk>\" # End of sequence word\n",
    "word_to_index[\"<unk>\"]=4\n",
    "word_to_vec_map[\"<unk>\"] = np.zeros((1,50))\n",
    "\n",
    "vocab_size = len(word_to_index)+1 #Since <pad> has been added (nothing existed at index = 0) and \"unk\" has been swapped\n",
    "\n",
    "#print(word_to_index[\"None\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dset)  3\n",
      "X_Indices.shape  (20, 40)\n",
      "Y_Indices.shape  (20, 5)\n",
      "dataset[0]  This is the third review\n",
      "dataset[1]  Awesome product\n",
      "\n",
      "Source after preprocessing (indices): [358160. 192973. 357266. 358029. 307253.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.]\n",
      "Target after preprocessing (indices): [ 64354. 292984.      0.      0.      0.]\n",
      "\n",
      "sentence x is  ['this', 'is', 'the', 'third', 'review', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "sentence y is  ['awesome', 'product', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Unit testing review & summary pre-processing \n",
    "\n",
    "dset = read_review_data('file_for_testing_pre_processing.csv',3)\n",
    "\n",
    "print(\"len(dset) \", len(dset))\n",
    "\n",
    "\n",
    "\n",
    "#dataset= [[X_data,Y_data]]\n",
    "#for i in range(1,m):\n",
    "    #dataset.append((X_data[i], Y_data[i]))\n",
    "\n",
    "X_Indices, Y_Indices = preprocess_data(dset, word_to_index, Tx, Ty,m)\n",
    "\n",
    "print(\"X_Indices.shape \", X_Indices.shape)\n",
    "print(\"Y_Indices.shape \", Y_Indices.shape)\n",
    "\n",
    "#print(\"X_Indices[0,0:10]  \", X_Indices[0][1])\n",
    "\n",
    "# print & check the source & target conversion to indices\n",
    "index = 2\n",
    "\n",
    "print(\"dataset[0] \", dset[index][0])\n",
    "print(\"dataset[1] \", dset[index][1])\n",
    "#print(\"Source date:\", dataset[index][0])\n",
    "#print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X_Indices[index])\n",
    "print(\"Target after preprocessing (indices):\", Y_Indices[index])\n",
    "print()\n",
    "\n",
    "print(\"sentence x is \", convert_indices_to_words(X_Indices[index],index_to_word))\n",
    "print(\"sentence y is \", convert_indices_to_words(Y_Indices[index],index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews loaded  2\n",
      "Number of reviews loaded  4\n",
      "Number of reviews loaded  6\n",
      "Number of reviews loaded  8\n",
      "Number of reviews loaded  10\n",
      "Number of reviews loaded  12\n",
      "Number of reviews loaded  14\n",
      "Number of reviews loaded  16\n",
      "Number of reviews loaded  18\n",
      "len(dataset)  20\n",
      "X_Indices.shape  (20, 40)\n",
      "Y_Indices.shape  (20, 5)\n",
      "dataset[0]  Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You\n",
      "dataset[1]  GREAT SWEET CANDY!\n",
      "\n",
      "Source after preprocessing (indices): [368318. 344429. 254258.  98992. 145839.  90792. 229835. 188481. 216635.\n",
      " 280746.  88126. 392261. 314370.  90755. 188692. 269953. 268046. 357266.\n",
      " 269091. 107031. 148889. 188481. 357266. 372191. 342019. 264937.  43010.\n",
      " 346136. 268046. 357266. 177567. 106170. 357266. 106170. 383514. 139954.\n",
      " 188481.  13194.  60665. 394521.]\n",
      "Target after preprocessing (indices): [166369. 349415.  90792.      0.      0.]\n",
      "\n",
      "sentence x is  ['twizzlers', 'strawberry', 'my', 'childhood', 'favorite', 'candy', 'made', 'in', 'lancaster', 'pennsylvania', 'by', 'y', 's', 'candies', 'inc', 'one', 'of', 'the', 'oldest', 'confectionery', 'firms', 'in', 'the', 'united', 'states', 'now', 'a', 'subsidiary', 'of', 'the', 'hershey', 'company', 'the', 'company', 'was', 'established', 'in', '1845', 'as', 'young']\n",
      "sentence y is  ['great', 'sweet', 'candy', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Load test & training data\n",
    "\n",
    "dataset = []\n",
    "#X_data, Y_data = read_review_data('product_reviews_modified.csv',m)\n",
    "\n",
    "dataset = read_review_data('product_reviews_modified.csv',m)\n",
    "# Remove the header rows\n",
    "#X_data.pop(0)\n",
    "#Y_data.pop(0)\n",
    "\n",
    "#print(\"Number of Reviews & Summaries Loaded = \", len(X_train), len(Y_train))\n",
    "\n",
    "#print(X_data[1], Y_data[1])\n",
    "#print(\"X_data shape is \", X_data.shape)\n",
    "\n",
    "print(\"len(dataset) \", len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "#dataset= [[X_data,Y_data]]\n",
    "#for i in range(1,m):\n",
    "    #dataset.append((X_data[i], Y_data[i]))\n",
    "\n",
    "X_Indices, Y_Indices = preprocess_data(dataset, word_to_index, Tx, Ty,m)\n",
    "\n",
    "print(\"X_Indices.shape \", X_Indices.shape)\n",
    "print(\"Y_Indices.shape \", Y_Indices.shape)\n",
    "\n",
    "#print(\"X_Indices[0,0:10]  \", X_Indices[0][1])\n",
    "\n",
    "# print & check the source & target conversion to indices\n",
    "index = m-1\n",
    "\n",
    "print(\"dataset[0] \", dataset[index][0])\n",
    "print(\"dataset[1] \", dataset[index][1])\n",
    "#print(\"Source date:\", dataset[index][0])\n",
    "#print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X_Indices[index])\n",
    "print(\"Target after preprocessing (indices):\", Y_Indices[index])\n",
    "print()\n",
    "\n",
    "print(\"sentence x is \", convert_indices_to_words(X_Indices[index],index_to_word))\n",
    "print(\"sentence y is \", convert_indices_to_words(Y_Indices[index],index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_dataset(Tx, Ty, vocab_size, m, X_Indices, Y_Indices):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    #for _ in range(m):\n",
    "    # generate source sequence\n",
    "    #source = generate_sequence(Tx, vocab_size)\n",
    "    source = X_Indices\n",
    "    # define padded target sequence\n",
    "    target = Y_Indices\n",
    "    #target.reverse()\n",
    "    # create padded input target sequence\n",
    "    #target_in = [0] + target[:-1]\n",
    "    target_in = np.insert(target, 0, 0, axis=-1)\n",
    "    target_in = target_in[:,0:Y_Indices.shape[1]]\n",
    "    \n",
    "    # encode\n",
    "    #src_encoded = to_categorical(source, num_classes=vocab_size)\n",
    "    #tar_encoded = to_categorical(target, num_classes=vocab_size)\n",
    "    #tar2_encoded = to_categorical(target_in, num_classes=vocab_size)\n",
    "    # store\n",
    "    X1 = source #np.array(src_encoded)\n",
    "    X2= target_in #np.array(tar2_encoded)\n",
    "    y= target #np.array(tar_encoded)\n",
    "    \n",
    "    #return array(X1), array(X2), array(y)\n",
    "    return X1, X2, y\n",
    "\n",
    "def get_dataset_one_hot(Tx, Ty, vocab_size, m, X_Indices, Y_Indices):\n",
    "    X1,X2,y = get_dataset(Tx, Ty, vocab_size, m, X_Indices, Y_Indices)\n",
    "    # encode\n",
    "    #X1_one_hot = to_categorical(X1, num_classes=vocab_size)\n",
    "    y_one_hot = to_categorical(y, num_classes=vocab_size)\n",
    "    X2_one_hot = to_categorical(X2, num_classes=vocab_size)\n",
    "    \n",
    "    return X1, X2_one_hot, y_one_hot\n",
    "    \n",
    "\n",
    "# load common Keras layers as objects\n",
    "def load_layers (n_input, n_output, n_units):\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    \n",
    "    return encoder,decoder_lstm,decoder_dense\n",
    "\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "\n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units,Ty, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    \n",
    "    encoder,decoder_lstm,decoder_dense = load_layers(n_input, n_output, n_units)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    #input_shape = (maxlen,) (in emojify example) which is further = Tx (Maximum words in the input sequence)\n",
    "    #sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    sentence_indices = Input(shape=(n_input,))\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    \n",
    "\n",
    "    # define training encoder\n",
    "    #encoder_inputs = Input(shape=(None, n_input))\n",
    "    \n",
    "    #encoder = LSTM(n_units, return_state=True)\n",
    "    \n",
    "    encoder_inputs = embeddings\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    \n",
    "    # define training decoder\n",
    "    #decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_sentence_indices = Input(shape=(Ty,n_output,))\n",
    "    \n",
    "    # 2nd input, the one being given to the decoder, should also be converted to word embeddings, to make it consistent in format with the encoder input\n",
    "    decoder_embeddings = embedding_layer(decoder_sentence_indices)\n",
    "    \n",
    "    decoder_inputs = decoder_embeddings\n",
    "\n",
    "    #decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    #decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "   \n",
    "    #model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model = Model([sentence_indices, decoder_sentence_indices], decoder_outputs)\n",
    "    \n",
    "    # define inference encoder\n",
    "    #encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    encoder_model = Model(sentence_indices, encoder_states)\n",
    "    \n",
    "    \n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    #decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    decoder_model = Model([decoder_sentence_indices] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict(source)\n",
    "\t# start of sequence input\n",
    "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(yhat[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_seq = yhat\n",
    "\treturn array(output)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test for Embedding layer set up\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "# Check the function. Expected output should be **weights[0][1][3] =**\t-0.3403\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 40) (20, 5, 400003) (20, 5, 400003)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension size must be evenly divisible by 4 but is 400003\n\tNumber of ways to split should evenly divide the split dimension for 'unified_lstm_5/split' (op: 'Split') with input shapes: [], [?,400003,512] and with computed input tensors: input[0] = <1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1818\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension size must be evenly divisible by 4 but is 400003\n\tNumber of ways to split should evenly divide the split dimension for 'unified_lstm_5/split' (op: 'Split') with input shapes: [], [?,400003,512] and with computed input tensors: input[0] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5d00048a750c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfenc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, metrics=['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2758beeaae62>\u001b[0m in \u001b[0;36mdefine_models\u001b[0;34m(n_input, n_output, n_units, Ty, word_to_vec_map, word_to_index)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m#decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;31m#decoder_dense = Dense(n_output, activation='softmax')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0moriginal_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m                       base_layer_utils.AutoAddUpdates(self,\n\u001b[1;32m    611\u001b[0m                                                       inputs)) as auto_updater:\n\u001b[0;32m--> 612\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m                 \u001b[0mauto_updater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   3269\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3270\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m               self.recurrent_activation, self.time_major)\n\u001b[0m\u001b[1;32m   3272\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m         \u001b[0;31m# Each time a `tf.function` is called, we will give it a unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, time_major)\u001b[0m\n\u001b[1;32m   3411\u001b[0m       \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m       \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3413\u001b[0;31m       input_length=timesteps)\n\u001b[0m\u001b[1;32m   3414\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   3503\u001b[0m     \u001b[0;31m# the value is discarded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     output_time_zero, _ = step_function(input_time_zero,\n\u001b[0;32m-> 3505\u001b[0;31m                                         initial_states + constants)\n\u001b[0m\u001b[1;32m   3506\u001b[0m     output_ta = tuple(\n\u001b[1;32m   3507\u001b[0m         tensor_array_ops.TensorArray(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   3395\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3397\u001b[0;31m     \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3399\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                 six.integer_types + (tensor_shape.Dimension,)):\n\u001b[1;32m   1523\u001b[0m     return gen_array_ops.split(\n\u001b[0;32m-> 1524\u001b[0;31m         axis=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msize_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(axis, value, num_split, name)\u001b[0m\n\u001b[1;32m   9336\u001b[0m   \u001b[0mnum_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_split\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9337\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 9338\u001b[0;31m         \"Split\", split_dim=axis, value=value, num_split=num_split, name=name)\n\u001b[0m\u001b[1;32m   9339\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9340\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    798\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    799\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    801\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m    450\u001b[0m     return super(FuncGraph, self).create_op(\n\u001b[1;32m    451\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         compute_device=compute_device)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3477\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3480\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1981\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1982\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1983\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension size must be evenly divisible by 4 but is 400003\n\tNumber of ways to split should evenly divide the split dimension for 'unified_lstm_5/split' (op: 'Split') with input shapes: [], [?,400003,512] and with computed input tensors: input[0] = <1>."
     ]
    }
   ],
   "source": [
    "# configure problem\n",
    "n_features = vocab_size\n",
    "n_steps_in = Tx\n",
    "n_steps_out = Ty\n",
    "\n",
    "# generate training dataset\n",
    "#X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "#X1, X2,y = get_dataset_one_hot(Tx, Ty, vocab_size, m, X_Indices, Y_Indices)\n",
    "\n",
    "\n",
    "#generating data set for \n",
    "\n",
    "X1, X2,y = get_dataset_one_hot(Tx, Ty, vocab_size, m, X_Indices, Y_Indices)\n",
    "print(X1.shape,X2.shape,y.shape)\n",
    "# train model\n",
    "\n",
    "\n",
    "# define model\n",
    "train, infenc, infdec = define_models(Tx, vocab_size, 128,Ty,word_to_vec_map, word_to_index)\n",
    "train.compile(optimizer='adam', loss='sparse_categorical_crossentropy')#, metrics=['acc'])\n",
    "train.summary()\n",
    "\n",
    "infenc.summary()\n",
    "\n",
    "infdec.summary()\n",
    "\n",
    "plot_model(train, to_file='model_train.png',show_shapes=True)\n",
    "plot_model(infenc, to_file='model_infenc.png',show_shapes=True)\n",
    "plot_model(infdec, to_file='model_infdec.png',show_shapes=True)\n",
    "#plot_model(encoder_model, to_file='encoder_model.png', show_shapes=True)\n",
    "#plot_model(decoder_model, to_file='decoder_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ce08ddae0a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_6 to have 2 dimensions, but got array with shape (20, 5, 400003)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92f3a90d5fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#train = load_model(\"seq2seq_model14_09_2019_20_19_06.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2594\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_6 to have 2 dimensions, but got array with shape (20, 5, 400003)"
     ]
    }
   ],
   "source": [
    "history = train.fit([X1, X2], y, epochs=1, batch_size=10, shuffle=True)\n",
    "\n",
    "#train = load_model(\"seq2seq_model14_09_2019_20_19_06.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"seq2seq_model\" + datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")+\".h5\"\n",
    "\n",
    "# Save the model\n",
    "#train.save(model_name)\n",
    "\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "#for _ in range(total):\n",
    "#X1, X2, y = get_dataset(Tx, Ty, vocab_size, m, X_Indices, Y_Indices)\n",
    "#target = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "#if array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "    #correct += 1\n",
    "#print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "#for _ in range(10):\n",
    "\t#X1, X2, y = get_dataset(Tx, Ty, vocab_size, m, X_Indices, Y_Indices)\n",
    "target = predict_sequence(infenc, infdec, X1[3:4,:,:], n_steps_out, n_features)\n",
    "print('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[2]), one_hot_decode(y[2]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[50320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[5]), one_hot_decode(y[5]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
